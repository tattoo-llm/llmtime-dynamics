{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e522ab-63ac-4d6f-9d3b-a9b851f069aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9713d9e7-5d3f-463b-9c6a-68950e2f0aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (1.4.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp39-cp39-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from scikit-learn) (3.2.0)\n",
      "Downloading scikit_learn-1.4.1.post1-cp39-cp39-macosx_12_0_arm64.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.4.0\n",
      "    Uninstalling scikit-learn-1.4.0:\n",
      "      Successfully uninstalled scikit-learn-1.4.0\n",
      "Successfully installed scikit-learn-1.4.1.post1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "!pip install -U scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5d314d-e9eb-4e41-a325-13d457ee9604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, verbose=False, delta=0):\n",
    "        self.patience = 3\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.use_multi_gpu = False\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            if self.verbose:\n",
    "                if (self.use_multi_gpu and self.local_rank == 0) or not self.use_multi_gpu:\n",
    "                    print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n",
    "            self.val_loss_min = val_loss\n",
    "            if self.use_multi_gpu:\n",
    "                if self.local_rank == 0:\n",
    "                    self.save_checkpoint(val_loss, model, path)\n",
    "                dist.barrier()\n",
    "            else:\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if (self.use_multi_gpu and self.local_rank == 0) or not self.use_multi_gpu:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            if self.use_multi_gpu:\n",
    "                if self.local_rank == 0:\n",
    "                    self.save_checkpoint(val_loss, model, path)\n",
    "                dist.barrier()\n",
    "            else:\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            if self.verbose:\n",
    "                if (self.use_multi_gpu and self.local_rank == 0) or not self.use_multi_gpu:\n",
    "                    print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n",
    "            self.val_loss_min = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        param_grad_dic = {\n",
    "        k: v.requires_grad for (k, v) in model.named_parameters()}\n",
    "        state_dict = model.state_dict()\n",
    "        for k in list(state_dict.keys()):\n",
    "            if k in param_grad_dic.keys() and not param_grad_dic[k]:\n",
    "                # delete parameters that do not require gradient\n",
    "                del state_dict[k]\n",
    "        torch.save(state_dict, path + '/' + f'checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5a1b8f-0f4c-4075-8b6f-df5e2f2d3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_ETT_hour(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None, data_path='ETTh1.csv',\n",
    "                 scale=True, seasonal_patterns=None, drop_short=False):\n",
    "        self.seq_len = size[0]\n",
    "        self.label_len = size[1]\n",
    "        self.pred_len = size[2]\n",
    "        self.token_len = self.seq_len - self.label_len\n",
    "        self.token_num = self.seq_len // self.token_len\n",
    "        self.flag = flag\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.read_data()\n",
    "        self.enc_in = self.data_x.shape[-1]\n",
    "        self.tot_len = len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def read_data(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        cols_data = df_raw.columns[1:]\n",
    "        df_data = df_raw[cols_data]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        data_name = self.data_path.split('.')[0]\n",
    "        self.data_stamp = torch.load(os.path.join(self.root_path, f'{data_name}.pt'))\n",
    "        self.data_stamp = self.data_stamp[border1:border2]\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feat_id = index // self.tot_len\n",
    "        s_begin = index % self.tot_len\n",
    "        \n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n",
    "        seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end:self.token_len]\n",
    "        seq_y_mark = self.data_stamp[s_end:r_end:self.token_len]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_x) - self.seq_len - self.pred_len + 1) * self.enc_in\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c50bc6c3-defb-4364-a97c-f16ffad642da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf26be2-d7bb-47ea-90cb-2dee7ea4721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Preprocess(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 data_path='ETTh1.csv', scale=True, seasonal_patterns=None):\n",
    "        self.seq_len = size[0]\n",
    "        self.label_len = size[1]\n",
    "        self.pred_len = size[2]\n",
    "        self.token_len = self.seq_len - self.label_len\n",
    "        self.token_num = self.seq_len // self.token_len\n",
    "        self.flag = flag\n",
    "        self.data_set_type = data_path.split('.')[0]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "        self.tot_len = len(self.data_stamp)\n",
    "\n",
    "    def __read_data__(self):\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path), nrows=5000)\n",
    "        df_stamp = df_raw[['date']]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date).apply(str)\n",
    "        self.data_stamp = df_stamp['date'].values\n",
    "        self.data_stamp = [str(x) for x in self.data_stamp]\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index % self.tot_len\n",
    "        s_end = s_begin + self.token_len\n",
    "        start = datetime.datetime.strptime(self.data_stamp[s_begin], \"%Y-%m-%d %H:%M:%S\")\n",
    "        if self.data_set_type in ['traffic', 'electricity', 'ETTh1', 'ETTh2']:\n",
    "            end = (start + datetime.timedelta(hours=self.token_len-1)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        elif self.data_set_type == 'weather':\n",
    "            end = (start + datetime.timedelta(minutes=10*(self.token_len-1))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        elif self.data_set_type in ['ETTm1', 'ETTm2']:\n",
    "            end = (start + datetime.timedelta(minutes=15*(self.token_len-1))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        seq_x_mark = f\"This is Time Series from {self.data_stamp[s_begin]} to {end}\"\n",
    "        return seq_x_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d779aa-29d3-421a-a3a3-fd18048c447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = Dataset_Preprocess(\n",
    "            root_path='./',\n",
    "            data_path='ETTm1.csv',\n",
    "            size=[672, 576, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b273d7e0-0b3d-430a-b19f-941e18030804",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b94a262-5cf4-40d0-a792-2edd251bc2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "!pip install transformers\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    ")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = 'cuda:0'\n",
    "        print(self.device)\n",
    "        \n",
    "        self.llama = LlamaForCausalLM.from_pretrained(\n",
    "            'meta-llama/Llama-2-7b-chat-hf',\n",
    "            device_map= self.device,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        self.llama_tokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "        self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token\n",
    "        self.vocab_size = self.llama_tokenizer.vocab_size\n",
    "        self.hidden_dim_of_llama = 4096\n",
    "        \n",
    "        for name, param in self.llama.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def tokenizer(self, x):\n",
    "        output = self.llama_tokenizer(x, return_tensors=\"pt\")['input_ids'].to(self.device)\n",
    "        result = self.llama.get_input_embeddings()(output)\n",
    "        return result   \n",
    "    \n",
    "    def forecast(self, x_mark_enc):        \n",
    "        # x_mark_enc: [bs x T x hidden_dim_of_llama]\n",
    "        x_mark_enc = torch.cat([self.tokenizer(x_mark_enc[i]) for i in range(len(x_mark_enc))], 0)\n",
    "        text_outputs = self.llama.model(inputs_embeds=x_mark_enc)[0]\n",
    "        text_outputs = text_outputs[:, -1, :]\n",
    "        return text_outputs\n",
    "    \n",
    "    def forward(self, x_mark_enc):\n",
    "        return self.forecast(x_mark_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69fb730a-2369-41e5-9aa9-7c44fa0ad491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rdey33/miniconda3/envs/llmtime/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "cuda:0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to request access at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/transformers/utils/hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/huggingface_hub/file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:302\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    299\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     )\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GatedRepoError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepoNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    305\u001b[0m     response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-65f0a6ab-6d00de276f15757b62073d7e;03b8b0e8-dc3d-4b33-847d-3bfcf3895c81)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nRepo model meta-llama/Llama-2-7b-chat-hf is gated. You must be authenticated to access it.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install accelerate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama_tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama_tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama_tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/transformers/modeling_utils.py:2926\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2925\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 2926\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2933\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2934\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2935\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2936\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2937\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2938\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2939\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2940\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2941\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llmtime/lib/python3.9/site-packages/transformers/utils/hub.py:400\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    386\u001b[0m         path_or_repo_id,\n\u001b[1;32m    387\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to request access at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`."
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "model = Model()\n",
    "model.to('cuda')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb18008-127f-45e6-ba8d-744084ea934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "519ccdc2-5b6d-463d-a800-d4842599c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373d45f93e8d418ca6abca3420c39f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m output_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(data_loader)):\n\u001b[0;32m----> 6\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(data)\n\u001b[1;32m      7\u001b[0m     output_list\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m      8\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(output_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(data_set.data_stamp))\n",
    "print(data_set.tot_len)\n",
    "save_dir_path = './'\n",
    "output_list = []\n",
    "for idx, data in tqdm(enumerate(data_loader)):\n",
    "    output = model(data)\n",
    "    output_list.append(output.detach().cpu())\n",
    "result = torch.cat(output_list, dim=0)\n",
    "print(result.shape)\n",
    "torch.save(result, save_dir_path + f'ETTm1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73955be8-0907-4a2c-9518-1bd53a4f64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Sort the batch by sequence length\n",
    "    batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "    # Extract sequences, targets, and marks\n",
    "    sequences, targets, seq_marks, target_marks = zip(*batch)\n",
    "    # Convert NumPy arrays to tensors\n",
    "    sequences = [torch.tensor(seq) for seq in sequences]\n",
    "    targets = [torch.tensor(target) for target in targets]\n",
    "    seq_marks = [torch.tensor(mark) for mark in seq_marks]\n",
    "    target_marks = [torch.tensor(mark) for mark in target_marks]\n",
    "    # Pad sequences\n",
    "    padded_sequences = rnn_utils.pad_sequence(sequences, batch_first=True)\n",
    "    padded_targets = rnn_utils.pad_sequence(targets, batch_first=True)\n",
    "    padded_seq_marks = rnn_utils.pad_sequence(seq_marks, batch_first=True)\n",
    "    padded_target_marks = rnn_utils.pad_sequence(target_marks, batch_first=True)\n",
    "    return padded_sequences, padded_targets, padded_seq_marks, padded_target_marks\n",
    "\n",
    "\n",
    "def data_provider(flag):\n",
    "    Data = Dataset_ETT_hour\n",
    "\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = False\n",
    "        batch_size = 1\n",
    "    elif flag == 'val':\n",
    "        shuffle_flag = True\n",
    "        drop_last = False\n",
    "        batch_size = 1\n",
    "    else:\n",
    "        shuffle_flag = True\n",
    "        drop_last = False\n",
    "        batch_size = 32\n",
    "\n",
    "    if flag in ['train', 'val']:\n",
    "        data_set = Data(\n",
    "            root_path='./',\n",
    "            data_path='ETTm1.csv',\n",
    "            flag=flag,\n",
    "            size=[672, 576, 96],\n",
    "            seasonal_patterns= 'Monthly',\n",
    "            drop_short=False,\n",
    "        )\n",
    "    else:\n",
    "        data_set = Data(\n",
    "            root_path='./',\n",
    "            data_path='ETTm1.csv',\n",
    "            flag=flag,\n",
    "            size=[672, 576, 96],\n",
    "            seasonal_patterns='Monthly',\n",
    "            drop_short=False,\n",
    "        )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=drop_last)\n",
    "    return data_set, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d653b-50ba-4f54-8f95-cdcd16c4616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_loader = data_provider('train')\n",
    "vali_data, vali_loader = data_provider(flag='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ab529-8e26-4ded-8327-67b3f9fe8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('.','./Model')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7099eeb-dba3-4715-bc38-372f7c440e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(train_loader)\n",
    "early_stopping = EarlyStopping(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecced6a5-229d-4b65-aeca-db20ad9d95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cb956-d03e-4399-bc8f-b9cb78e9c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer perceptron to encode/decode high dimension representation of sequential data\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 f_in, \n",
    "                 f_out, \n",
    "                 hidden_dim=256, \n",
    "                 hidden_layers=2, \n",
    "                 dropout=0.1,\n",
    "                 activation='tanh'): \n",
    "        super(MLP, self).__init__()\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout = dropout\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        layers = [nn.Linear(self.f_in, self.hidden_dim), \n",
    "                  self.activation, nn.Dropout(self.dropout)]\n",
    "        for i in range(self.hidden_layers-2):\n",
    "            layers += [nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                       self.activation, nn.Dropout(dropout)]\n",
    "        \n",
    "        layers += [nn.Linear(hidden_dim, f_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:     B x S x f_in\n",
    "        # y:     B x S x f_out\n",
    "        # x = x.to(torch.float16)\n",
    "        y = self.layers(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c1e06-2a49-47be-b0a1-d63a58636d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load llama2\n",
    "from torch import cuda, bfloat16\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfbd66-a752-4981-b1a4-a29d819d4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.token_len = 96\n",
    "        self.device = torch.device('cuda:{}'.format(0))\n",
    "        print(self.device)\n",
    "        # Load llama2\n",
    "        bnb_config = transformers.BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_quant_type='nf4',\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_compute_dtype=bfloat16 )\n",
    "\n",
    "\n",
    "        \n",
    "        self.llama = LlamaForCausalLM.from_pretrained(\n",
    "            'meta-llama/Llama-2-7b-chat-hf',\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=self.device,\n",
    "        )\n",
    "        self.hidden_dim_of_llama = 4096\n",
    "        #self.llama.half() \n",
    "        self.mix = False\n",
    "        if self.mix:\n",
    "            self.add_scale = nn.Parameter(torch.ones([]))\n",
    "        \n",
    "        for name, param in self.llama.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.encoder = MLP(self.token_len, self.hidden_dim_of_llama, \n",
    "                            256, 2, 0.1, 'tanh')\n",
    "        self.decoder = MLP(self.hidden_dim_of_llama, self.token_len, 256, 2, 0.1, 'tanh') \n",
    "    \n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        means = x_enc.mean(1, keepdim=True).detach()    \n",
    "        x_enc = x_enc - means\n",
    "        stdev = torch.sqrt(\n",
    "            torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "        x_enc /= stdev\n",
    "        \n",
    "        bs, _, n_vars = x_enc.shape\n",
    "        # x_enc: [bs x nvars x seq_len]\n",
    "        x_enc = x_enc.permute(0, 2, 1)\n",
    "        # x_enc: [bs * nvars x seq_len]\n",
    "        x_enc = x_enc.reshape(x_enc.shape[0] * x_enc.shape[1], -1)\n",
    "        # fold_out: [bs * n_vars x token_num x token_len]\n",
    "        fold_out = x_enc.unfold(dimension=-1, size=self.token_len, step=self.token_len)\n",
    "        token_num = fold_out.shape[1]\n",
    "        # times_embeds: [bs * n_vars x token_num x hidden_dim_of_llama]\n",
    "        times_embeds = self.encoder(fold_out)\n",
    "        if self.mix:\n",
    "            times_embeds = times_embeds / times_embeds.norm(dim=2, keepdim=True)\n",
    "            x_mark_enc = x_mark_enc / x_mark_enc.norm(dim=2, keepdim=True)\n",
    "            times_embeds = times_embeds + self.add_scale * x_mark_enc\n",
    "        # outputs: [bs * n_vars x token_num x hidden_dim_of_llama]\n",
    "        outputs = self.llama.model(\n",
    "            inputs_embeds=times_embeds)[0]\n",
    "        # dec_out: [bs * n_vars x token_num x token_len]\n",
    "        dec_out = self.decoder(outputs)\n",
    "        dec_out = dec_out.reshape(bs, n_vars, -1)\n",
    "        # dec_out: [bs x token_num * token_len x n_vars]\n",
    "        dec_out = dec_out.permute(0, 2, 1)\n",
    "        \n",
    "        dec_out = dec_out * \\\n",
    "            (stdev[:, 0, :].unsqueeze(1).repeat(1, token_num * self.token_len, 1))\n",
    "        dec_out = dec_out + \\\n",
    "            (means[:, 0, :].unsqueeze(1).repeat(1, token_num * self.token_len, 1))\n",
    "        \n",
    "        return dec_out\n",
    "    \n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # x_enc = x_enc.to(torch.float16)\n",
    "        # x_mark_enc = x_mark_enc.to(torch.float16)\n",
    "        # # x_dec = x_dec.to(torch.float16)\n",
    "        # x_mark_dec = x_mark_dec.to(torch.float16)\n",
    "        return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04e5c6-701f-4e89-9eff-576c3e88f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:{}'.format(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6a2a8-8ec9-42f7-881b-19bbce43706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b546b-a7eb-40ac-b8c6-4eddc49f2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4c732-ea3c-4a0c-b78c-a025ccda799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = []\n",
    "for n, p in model.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    else:\n",
    "        p_list.append(p)\n",
    "        print(n, p.dtype, p.shape)\n",
    "model_optim = optim.Adam([{'params': p_list}], lr=0.0001, weight_decay= 0)\n",
    "print('next learning rate is {}'.format(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da0ea5-0145-4feb-be5b-6613fd9a88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=10, eta_min=1e-8)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40516fa-a2c9-4358-be26-1a74618d42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Directory to save models\n",
    "# save_dir = 'models'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     iter_count = 0\n",
    "#     loss_val = torch.tensor(0., device=device)\n",
    "#     count = torch.tensor(0., device=device)\n",
    "#     model.train()\n",
    "#     epoch_time = time.time()\n",
    "    \n",
    "#     # Wrap train_loader with tqdm for progress tracking\n",
    "#     train_loader_iter = tqdm(train_loader, desc=\"loss\", leave=False)\n",
    "    \n",
    "#     for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader_iter):\n",
    "#         # gc.collect()\n",
    "#         iter_count += 1\n",
    "#         time_now = time.time()\n",
    "#         model_optim.zero_grad()\n",
    "#         batch_x = batch_x.float().to(device)\n",
    "#         batch_y = batch_y.float().to(device)\n",
    "#         batch_x_mark = batch_x_mark.float().to(device)\n",
    "#         batch_y_mark = batch_y_mark.float().to(device)\n",
    "       \n",
    "#         outputs = model(batch_x, batch_x_mark, None, batch_y_mark)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "        \n",
    "#         loss_val += loss\n",
    "#         count += 1\n",
    "        \n",
    "#         if (i + 1) % 100 == 0:\n",
    "#             speed = (time.time() - time_now) / iter_count\n",
    "#             left_time = speed * ((1 - epoch) * train_steps - i)\n",
    "#             print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "#             iter_count = 0\n",
    "#             time_now = time.time()\n",
    "            \n",
    "#             # Save the model\n",
    "#             save_path = os.path.join(save_dir, f'model_epoch{epoch}_iter{i}.pt')\n",
    "#             torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "#     train_loss = loss_val.item() / count.item()\n",
    "#     print(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64a663-71cb-4232-b172-6ab55659ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_loader = data_provider(flag='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776197d-4feb-489c-86d2-7ffa302bfdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(test_loader.dataset)\n",
    "\n",
    "# Calculate the number of samples to include (50% of the total)\n",
    "num_samples_to_include = total_samples // 10\n",
    "\n",
    "# Create a SubsetRandomSampler to select the first half of the data\n",
    "subset_indices = list(range(num_samples_to_include))\n",
    "subset_sampler = torch.utils.data.sampler.SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# Create a new DataLoader using the SubsetRandomSampler\n",
    "half_test_loader = torch.utils.data.DataLoader(test_loader.dataset, \n",
    "                                               batch_size=test_loader.batch_size,\n",
    "                                               sampler=subset_sampler,\n",
    "                                               num_workers=test_loader.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36174a89-9ef8-498b-96c5-8305514c2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('models', f'model_epoch{0}_iter{99}.pt')\n",
    "load_item = torch.load(model_path)\n",
    "state_dict = {k.replace('module.', ''): v for k, v in load_item.items()}\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388f7cc-98b6-45e3-886f-18eebf1e656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "folder_path = './test_results/' + '/'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "time_now = time.time()\n",
    "test_steps = len(test_loader)\n",
    "print(test_steps)\n",
    "iter_count = 0\n",
    "model.eval()\n",
    "\n",
    "# Wrap test_loader with tqdm for progress tracking\n",
    "test_loader_iter = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(half_test_loader):\n",
    "        iter_count += 1\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "        inference_steps = 1\n",
    "        dis = 96 - inference_steps * 96\n",
    "        if dis != 0:\n",
    "            inference_steps += 1\n",
    "        pred_y = []\n",
    "        for j in range(inference_steps):\n",
    "            if len(pred_y) != 0:\n",
    "                batch_x = torch.cat([batch_x[:, 96:, :], pred_y[-1]], dim=1)\n",
    "                tmp = batch_y_mark[:, j-1:j, :]\n",
    "                batch_x_mark = torch.cat([batch_x_mark[:, 1:, :], tmp], dim=1)\n",
    "            outputs = model(batch_x, batch_x_mark, None, batch_y_mark)\n",
    "            pred_y.append(outputs[:, -96:, :])\n",
    "        pred_y = torch.cat(pred_y, dim=1)\n",
    "        if dis != 0:\n",
    "            pred_y = pred_y[:, :-dis, :]\n",
    "        batch_y = batch_y[:, -96:, :].to(device)\n",
    "        outputs = pred_y.detach().cpu()\n",
    "        batch_y = batch_y.detach().cpu()\n",
    "        pred = outputs\n",
    "        true = batch_y\n",
    "\n",
    "        preds.append(pred)\n",
    "        trues.append(true)\n",
    "        break  \n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    trues = torch.cat(trues, dim=0).numpy()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a93a9-8531-43e4-b23b-6c6d369ecb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046756d8-18ca-4f77-891c-cb7fc73c11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot and save preds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(preds[0], label='Predictions', color='blue')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('predicted_values.png')  # Save the plot as an image\n",
    "plt.show()\n",
    "\n",
    "# Plot and save trues\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trues[0], label='True Values', color='green')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('True Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('true_values.png')  # Save the plot as an image\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17033536-d927-4408-bd96-de5512710922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
